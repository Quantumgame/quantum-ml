{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples : 76700\n",
      "Training samples : 61360\n",
      "Test samples : 15340\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1117e3390>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.20043, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0331472.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:50:50\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:50:51\n",
      "INFO:tensorflow:Saving dict for global step 100: Train accuracy = 0.91, global_step = 100, loss = 0.0332547\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:50:52\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:50:53\n",
      "INFO:tensorflow:Saving dict for global step 100: Val accuracy = 0.35, global_step = 100, loss = 0.0921645\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0429575, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0287729.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:08\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:08\n",
      "INFO:tensorflow:Saving dict for global step 200: Train accuracy = 0.91, global_step = 200, loss = 0.0325334\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:10\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:10\n",
      "INFO:tensorflow:Saving dict for global step 200: Val accuracy = 0.7, global_step = 200, loss = 0.0771674\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0351447, step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0208343.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:24\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-300\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:25\n",
      "INFO:tensorflow:Saving dict for global step 300: Train accuracy = 0.95, global_step = 300, loss = 0.0247073\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:26\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-300\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:26\n",
      "INFO:tensorflow:Saving dict for global step 300: Val accuracy = 0.76, global_step = 300, loss = 0.0702673\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-300\n",
      "INFO:tensorflow:Saving checkpoints for 301 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0326743, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0174693.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:41\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-400\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:42\n",
      "INFO:tensorflow:Saving dict for global step 400: Train accuracy = 0.97, global_step = 400, loss = 0.0172775\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:43\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-400\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:51:43\n",
      "INFO:tensorflow:Saving dict for global step 400: Val accuracy = 0.8, global_step = 400, loss = 0.0600929\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-400\n",
      "INFO:tensorflow:Saving checkpoints for 401 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0303073, step = 401\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0140794.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:51:59\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-500\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:00\n",
      "INFO:tensorflow:Saving dict for global step 500: Train accuracy = 0.92, global_step = 500, loss = 0.0139776\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:01\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-500\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:01\n",
      "INFO:tensorflow:Saving dict for global step 500: Val accuracy = 0.77, global_step = 500, loss = 0.0636939\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-500\n",
      "INFO:tensorflow:Saving checkpoints for 501 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0239492, step = 501\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0125608.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:15\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-600\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:16\n",
      "INFO:tensorflow:Saving dict for global step 600: Train accuracy = 0.96, global_step = 600, loss = 0.010797\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:17\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-600\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:17\n",
      "INFO:tensorflow:Saving dict for global step 600: Val accuracy = 0.9, global_step = 600, loss = 0.0459864\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-600\n",
      "INFO:tensorflow:Saving checkpoints for 601 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0160858, step = 601\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0121937.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:32\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-700\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:33\n",
      "INFO:tensorflow:Saving dict for global step 700: Train accuracy = 0.96, global_step = 700, loss = 0.0129306\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:34\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-700\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:35\n",
      "INFO:tensorflow:Saving dict for global step 700: Val accuracy = 0.91, global_step = 700, loss = 0.0505728\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-700\n",
      "INFO:tensorflow:Saving checkpoints for 701 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0199894, step = 701\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0137928.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:50\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-800\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:50\n",
      "INFO:tensorflow:Saving dict for global step 800: Train accuracy = 0.93, global_step = 800, loss = 0.00846842\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:52:51\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-800\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:52:52\n",
      "INFO:tensorflow:Saving dict for global step 800: Val accuracy = 0.87, global_step = 800, loss = 0.0561812\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-800\n",
      "INFO:tensorflow:Saving checkpoints for 801 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0134452, step = 801\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00880878.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:53:07\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-900\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:53:07\n",
      "INFO:tensorflow:Saving dict for global step 900: Train accuracy = 0.95, global_step = 900, loss = 0.0119307\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:53:08\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-900\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:53:09\n",
      "INFO:tensorflow:Saving dict for global step 900: Val accuracy = 0.92, global_step = 900, loss = 0.0402989\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-900\n",
      "INFO:tensorflow:Saving checkpoints for 901 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0154856, step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0105401.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:53:23\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:53:23\n",
      "INFO:tensorflow:Saving dict for global step 1000: Train accuracy = 0.94, global_step = 1000, loss = 0.0582619\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:53:24\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:53:25\n",
      "INFO:tensorflow:Saving dict for global step 1000: Val accuracy = 0.94, global_step = 1000, loss = 0.0352204\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-10-19:53:44\n",
      "INFO:tensorflow:Restoring parameters from /Users/ssk4/tensorflow_models/cnn_prob_test3/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-10-19:53:51\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.940156, global_step = 1000, loss = 0.0214549\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "Training done in 195.26118683815002 seconds.\n"
     ]
    }
   ],
   "source": [
    "# learning using a CNN\n",
    "\n",
    "# CNN for learning!\n",
    "\n",
    "# learn the states of a double dot\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# application logic will be added here\n",
    "def cnn_model_fn(features,labels,mode):\n",
    "    '''Model function for CNN'''\n",
    "    #input layer\n",
    "    input_layer = tf.cast(tf.reshape(features,[-1,30,30,1]),tf.float32)\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                            filters=8,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    #conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "    #                        filters=16,\n",
    "    #                        kernel_size=[5,5],\n",
    "    #                        padding=\"same\",\n",
    "    #                        activation=tf.nn.relu)\n",
    "    #pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(inputs=pool1)\n",
    "    # dense output layer\n",
    "    out1 = tf.layers.dense(inputs=flat,units=64,activation=tf.nn.relu)  \n",
    "    dropout1 = tf.layers.dropout(\n",
    "      inputs=out1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n",
    "    \n",
    "    out = tf.layers.dense(inputs=dropout1, units=4)\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # Calculate loss( for both TRAIN AND EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        loss = tf.losses.mean_squared_error(out,labels['prob'])\n",
    "\n",
    "    # Configure the training op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=1e-3,\n",
    "            optimizer=tf.train.AdamOptimizer)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions= {\n",
    "        \"prob\" : out,\n",
    "        \"states\" : tf.argmax(out,axis=1),\n",
    "    }\n",
    "    \n",
    "    # Returna  ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(mode=mode,predictions=predictions,loss=loss, train_op=train_op)\n",
    "    \n",
    "def get_train_inputs():\n",
    "    n_batch = 100\n",
    "    index = np.random.choice(n_train,n_batch,replace=False)\n",
    "    inp = []\n",
    "    oup = []\n",
    "    for i in index:\n",
    "        dat = np.load(files[i])\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "\n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "def get_val_inputs():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:(n_train + 100)]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "def get_test_inputs():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    labels_dict['prob'] = y\n",
    "    labels_dict['states'] = tf.argmax(y,axis=1)\n",
    "    return x,labels_dict\n",
    "\n",
    "# get the data\n",
    "data_folder_path = \"/Users/ssk4/Downloads/data_subimage/\"\n",
    "files = glob.glob(data_folder_path + \"*.npy\")\n",
    "\n",
    "import random\n",
    "files = files[:]\n",
    "\n",
    "n_samples = len(files)\n",
    "train_sample_ratio = 0.8\n",
    "n_train = int(train_sample_ratio * n_samples)\n",
    "\n",
    "print(\"Total number of samples :\",n_samples)\n",
    "print(\"Training samples :\",n_train)\n",
    "print(\"Test samples :\",n_samples - n_train)\n",
    "\n",
    "st = time.time()\n",
    "# create the estimator\n",
    "dd_classifier = learn.Estimator(model_fn=cnn_model_fn,model_dir=\"/Users/ssk4/tensorflow_models/cnn_prob_test3/\")\n",
    "\n",
    "n_epochs = 10\n",
    "steps_per_epoch = 100\n",
    "for _ in range(n_epochs):\n",
    "    dd_classifier.fit(\n",
    "        input_fn=get_train_inputs,\n",
    "        steps=steps_per_epoch)\n",
    "\n",
    "    train_metrics = {\n",
    "        \"Train accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "    }\n",
    "    dd_classifier.evaluate(input_fn=get_train_inputs,metrics=train_metrics,steps=1)\n",
    "\n",
    "    val_metrics = {\n",
    "        \"Val accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "    }\n",
    "    dd_classifier.evaluate(input_fn=get_val_inputs,metrics=val_metrics,steps=1)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "}\n",
    "dd_classifier.evaluate(input_fn=get_test_inputs,metrics=metrics,steps=1)\n",
    "print(\"Training done in\",time.time()-st,\"seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-167-99311670a755>:22: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hs/nm7qn85519v78y5bbl49h61h001sq3/T/tmpnel54pgp/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def accr_test_input_fn():\n",
    "    inp = []\n",
    "    oup = []\n",
    "       \n",
    "    for file in files[n_train:(n_train + 100)]:\n",
    "        dat = np.load(file)\n",
    "        inp += [dat.item()['current_map']]\n",
    "        oup += [dat.item()['label']]\n",
    "    \n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    oup = np.array(oup,dtype=np.float32)\n",
    "    \n",
    "    x = tf.constant(inp)\n",
    "    y = tf.constant(oup)\n",
    "    return x,y\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\"),\n",
    "}\n",
    "\n",
    "pred = dd_classifier.predict(input_fn=accr_test_input_fn,as_iterable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-07-01-17:55:59\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/hs/nm7qn85519v78y5bbl49h61h001sq3/T/tmp7me_3mgw/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-01-17:56:00\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.93, global_step = 100, loss = 0.0134305\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93000001, 'global_step': 100, 'loss': 0.013430518}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"accuracy\" : learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"states\",label_key=\"states\"),\n",
    "}\n",
    "dd_classifier.evaluate(input_fn=get_train_inputs,metrics=metrics,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3,\n",
       "       2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2,\n",
       "       2, 2, 3, 2, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.53586641e-03,   5.17740846e-03,   9.50941443e-01,\n",
       "          2.10614204e-02],\n",
       "       [  1.37775205e-03,   3.36754322e-03,   9.73362803e-01,\n",
       "          9.35178995e-03],\n",
       "       [  5.08314930e-04,  -4.10084426e-03,   7.75267899e-01,\n",
       "          2.43259087e-01],\n",
       "       [  9.66005959e-04,  -1.75612420e-03,   6.38581038e-01,\n",
       "          3.70760769e-01],\n",
       "       [  4.98790294e-04,   2.79003754e-03,   1.00814617e+00,\n",
       "         -4.23276424e-03],\n",
       "       [  8.87231901e-04,   8.87972862e-03,   2.14482144e-01,\n",
       "          7.55513906e-01],\n",
       "       [  8.70760530e-04,  -5.79632074e-03,   6.50480747e-01,\n",
       "          3.68960559e-01],\n",
       "       [  4.69686463e-04,   6.43382221e-03,   9.58739400e-01,\n",
       "          3.81074250e-02],\n",
       "       [  5.87210059e-04,   2.56353058e-02,   1.01916778e+00,\n",
       "         -2.43719220e-02],\n",
       "       [  3.05661000e-04,   2.05680728e-04,   3.28062057e-01,\n",
       "          6.84476078e-01],\n",
       "       [  2.39092670e-03,   1.54843740e-02,   8.87747407e-01,\n",
       "          5.56392074e-02],\n",
       "       [  2.58115120e-04,  -1.05108246e-02,   1.05984473e+00,\n",
       "         -1.71020031e-02],\n",
       "       [  7.47594051e-04,  -4.11675125e-03,   1.01621664e+00,\n",
       "         -2.63503194e-03],\n",
       "       [ -5.66071831e-04,   5.56235760e-03,   2.25614011e-02,\n",
       "          9.82613742e-01],\n",
       "       [  4.17692587e-04,   3.20246443e-03,   2.74223447e-01,\n",
       "          7.24481761e-01],\n",
       "       [  3.87709588e-06,   1.26091704e-01,   9.54131484e-01,\n",
       "         -3.64763737e-02],\n",
       "       [  5.67901693e-03,   1.02226205e-01,   9.67179298e-01,\n",
       "         -4.03436422e-02],\n",
       "       [  1.25361793e-03,   6.49347156e-03,   9.68709469e-01,\n",
       "          1.40236765e-02],\n",
       "       [  1.66207738e-03,   4.42840159e-03,   6.51361585e-01,\n",
       "          3.19475412e-01],\n",
       "       [  9.28110443e-04,   1.39953867e-02,   9.71236587e-01,\n",
       "          3.21336091e-03],\n",
       "       [  8.15638341e-04,   8.72313976e-04,   1.00527418e+00,\n",
       "          2.56505609e-03],\n",
       "       [ -3.06621194e-04,   1.02197900e-02,   3.20366323e-02,\n",
       "          9.60138202e-01],\n",
       "       [  2.26744171e-03,   4.32547182e-02,   6.00103855e-01,\n",
       "          3.56535316e-01],\n",
       "       [  1.46848336e-03,   2.23178528e-02,   9.64352489e-01,\n",
       "          9.38296318e-04],\n",
       "       [  1.34263746e-03,   1.36427954e-03,   9.55325365e-01,\n",
       "          2.65608281e-02],\n",
       "       [  1.64419785e-03,   9.45420563e-03,   9.63532090e-01,\n",
       "          1.35118663e-02],\n",
       "       [ -3.63837928e-04,  -8.17778707e-03,   1.04796076e+00,\n",
       "          1.25220269e-02],\n",
       "       [  2.86182761e-03,   1.72755197e-02,   7.27676630e-01,\n",
       "          1.90096512e-01],\n",
       "       [  8.24103132e-04,  -1.44651532e-03,   1.01151967e+00,\n",
       "         -3.01909447e-03],\n",
       "       [  1.16623482e-02,   8.04407001e-02,   8.02424312e-01,\n",
       "          5.67801893e-02],\n",
       "       [  1.11796446e-02,   2.89719820e-01,   8.02401662e-01,\n",
       "         -1.82116628e-02],\n",
       "       [  6.71632588e-04,   1.36267766e-03,   9.92137313e-01,\n",
       "          5.69683313e-03],\n",
       "       [ -1.83896162e-04,   3.15372273e-02,   1.00493324e+00,\n",
       "         -5.27173281e-03],\n",
       "       [  2.45934539e-03,   1.24983378e-02,   8.47004533e-01,\n",
       "          9.50867534e-02],\n",
       "       [ -2.38071196e-04,   1.44472897e-01,   8.95504594e-01,\n",
       "         -7.60203600e-03],\n",
       "       [  1.00167096e-03,   1.79855190e-02,   2.92317271e-01,\n",
       "          6.63602710e-01],\n",
       "       [  5.68266027e-04,  -1.72440708e-03,   4.34956253e-01,\n",
       "          5.76852620e-01],\n",
       "       [ -5.07383607e-04,   4.35791165e-03,   3.51708829e-02,\n",
       "          9.70925450e-01],\n",
       "       [  8.08550045e-04,  -6.92734867e-03,   9.43309426e-01,\n",
       "          8.00341964e-02],\n",
       "       [  8.17167573e-04,  -1.53177977e-03,   1.00094175e+00,\n",
       "          2.95725465e-03],\n",
       "       [ -3.59052792e-04,   8.41400027e-03,   2.91620195e-02,\n",
       "          9.66757655e-01],\n",
       "       [  1.01063494e-03,   4.03824449e-03,   9.69061017e-01,\n",
       "          1.39303952e-02],\n",
       "       [  6.59182668e-04,   4.76231426e-03,   9.96148348e-01,\n",
       "         -2.58946419e-03],\n",
       "       [  1.54439174e-03,   4.17821109e-03,   5.64432800e-01,\n",
       "          4.18140084e-01],\n",
       "       [  1.63041428e-03,   3.92793864e-03,   8.72108579e-01,\n",
       "          9.66561735e-02],\n",
       "       [  4.07937914e-04,  -2.67503411e-03,   7.89289117e-01,\n",
       "          2.33141661e-01],\n",
       "       [  1.24827586e-03,   1.78275257e-03,   9.70643878e-01,\n",
       "          1.12838149e-02],\n",
       "       [  8.47622752e-04,   2.92966515e-03,   9.58679199e-01,\n",
       "          2.51457095e-02],\n",
       "       [  2.52182875e-03,   1.18899792e-02,   8.31291556e-01,\n",
       "          9.44815576e-02],\n",
       "       [  7.08202831e-04,   8.26808065e-02,   9.37207699e-01,\n",
       "         -1.22717023e-03],\n",
       "       [ -6.54630363e-04,   4.77458909e-03,   1.62093341e-02,\n",
       "          9.92393732e-01],\n",
       "       [  9.25727189e-04,  -5.83702326e-03,   9.43114758e-01,\n",
       "          7.68313110e-02],\n",
       "       [  1.77377835e-04,  -1.93938613e-05,   6.74325645e-01,\n",
       "          3.46136212e-01],\n",
       "       [  7.21039250e-04,  -7.14695454e-03,   1.00439179e+00,\n",
       "          1.13523006e-02],\n",
       "       [  1.07571203e-03,   2.20552459e-03,   9.81594086e-01,\n",
       "          8.79433751e-03],\n",
       "       [  1.03669269e-02,   1.17477894e-01,   9.61061358e-01,\n",
       "         -3.01026106e-02],\n",
       "       [  3.97859141e-04,  -8.49153101e-03,   8.61365318e-01,\n",
       "          1.79599836e-01],\n",
       "       [ -5.23767062e-03,  -3.20372283e-02,   1.34997523e+00,\n",
       "         -1.09149337e-01],\n",
       "       [  7.23217893e-03,   1.61800981e-01,   9.02897954e-01,\n",
       "         -3.21997404e-02],\n",
       "       [  1.83595344e-04,  -1.30607635e-02,   1.08201849e+00,\n",
       "         -3.07914913e-02],\n",
       "       [  1.26403477e-03,   1.47341490e-02,   9.52536464e-01,\n",
       "          1.08629912e-02],\n",
       "       [  1.83034688e-04,   1.85946375e-03,   2.95656145e-01,\n",
       "          7.13636756e-01],\n",
       "       [ -3.20533291e-05,  -3.28697264e-03,   1.06046212e+00,\n",
       "         -2.46390998e-02],\n",
       "       [  1.62732601e-03,   7.20009208e-03,   5.19666493e-01,\n",
       "          4.55618709e-01],\n",
       "       [ -2.04388052e-05,   3.34842876e-03,   1.68870851e-01,\n",
       "          8.32732916e-01],\n",
       "       [  1.00294501e-03,   4.48520482e-03,   7.33243346e-01,\n",
       "          2.47217700e-01],\n",
       "       [  4.06623073e-03,   9.92218778e-03,   6.50843263e-01,\n",
       "          2.92543739e-01],\n",
       "       [  8.47009011e-04,  -2.74063647e-03,   1.01201582e+00,\n",
       "         -2.60227919e-03],\n",
       "       [ -5.79264015e-04,   4.55665588e-03,   2.12527215e-02,\n",
       "          9.85547960e-01],\n",
       "       [  3.16131860e-03,   1.49658844e-02,   4.97868210e-01,\n",
       "          4.19808507e-01],\n",
       "       [  2.49119103e-03,   1.36102661e-02,   7.95333624e-01,\n",
       "          1.04538321e-01],\n",
       "       [ -5.54055907e-04,   5.73243201e-03,   2.35709250e-02,\n",
       "          9.81115162e-01],\n",
       "       [  1.40285492e-03,  -1.43051147e-06,   8.40638518e-01,\n",
       "          1.48132652e-01],\n",
       "       [  2.33384222e-03,   1.88882276e-03,   6.79910898e-01,\n",
       "          2.96425372e-01],\n",
       "       [  1.26260892e-03,   6.04952872e-03,   6.13144159e-01,\n",
       "          3.69573206e-01],\n",
       "       [  8.98027793e-04,   2.28247792e-03,   6.01194859e-01,\n",
       "          3.95992905e-01],\n",
       "       [ -7.52207637e-03,   3.26109916e-01,   7.36782074e-01,\n",
       "         -1.23046935e-02],\n",
       "       [  1.58115663e-03,   4.40952592e-02,   5.99486649e-01,\n",
       "          3.05624723e-01],\n",
       "       [  7.64749013e-04,   2.11306289e-03,   9.66441870e-01,\n",
       "          3.26219201e-02],\n",
       "       [  1.63781270e-03,   1.02468207e-02,   5.94302654e-01,\n",
       "          3.65747094e-01],\n",
       "       [  8.90163705e-04,  -2.13201344e-03,   9.91318226e-01,\n",
       "          8.96710157e-03],\n",
       "       [  1.67580508e-03,   9.96024907e-03,   5.45021713e-01,\n",
       "          4.16310281e-01],\n",
       "       [  6.04310073e-04,   5.95795363e-03,   9.24239993e-01,\n",
       "          6.98844492e-02],\n",
       "       [ -6.25185668e-04,   2.94831395e-03,   7.38100708e-02,\n",
       "          9.40092266e-01],\n",
       "       [  1.12474617e-03,   8.64905864e-03,   9.17384505e-01,\n",
       "          5.88173866e-02],\n",
       "       [  2.42106989e-03,   2.10845806e-02,   8.15902233e-01,\n",
       "          7.98137486e-02],\n",
       "       [ -1.94998458e-04,   5.79770282e-03,   1.30781114e-01,\n",
       "          8.70872855e-01],\n",
       "       [  5.25278971e-04,  -5.10852784e-03,   1.01711786e+00,\n",
       "          2.37151980e-03],\n",
       "       [  8.82014632e-04,   6.35430217e-04,   4.19752061e-01,\n",
       "          5.80813169e-01],\n",
       "       [  1.24268234e-03,   4.47004475e-02,   8.95658612e-01,\n",
       "          4.41329032e-02],\n",
       "       [  1.35124931e-02,   2.19580419e-02,   9.09970284e-01,\n",
       "          1.43587649e-01],\n",
       "       [  9.44587402e-04,  -2.26093084e-03,   9.83043432e-01,\n",
       "          2.25315243e-02],\n",
       "       [  5.47110103e-04,  -3.70258093e-03,   1.02321124e+00,\n",
       "         -1.02322996e-02],\n",
       "       [  9.21153463e-04,   7.14763999e-03,   9.80939984e-01,\n",
       "          9.15244222e-04],\n",
       "       [ -5.19853085e-04,   3.88704985e-03,   9.76406336e-02,\n",
       "          9.14453030e-01],\n",
       "       [  8.80877860e-03,   3.46845061e-01,   6.71135008e-01,\n",
       "         -1.01506412e-02],\n",
       "       [  7.11125694e-03,   9.09093097e-02,   8.95801187e-01,\n",
       "          8.62199068e-03],\n",
       "       [  1.09204184e-03,   5.98566607e-03,   9.88742352e-01,\n",
       "         -2.36272812e-03],\n",
       "       [  3.83276492e-04,   3.76834720e-03,   2.39822790e-01,\n",
       "          7.54230857e-01],\n",
       "       [  9.77162272e-04,   7.61814043e-03,   2.61006474e-01,\n",
       "          7.14003205e-01]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.69222224  0.30777779]\n",
      " [ 0.          0.          0.50222224  0.49777779]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.18444444  0.81555557]\n",
      " [ 0.          0.          0.52888888  0.47111112]\n",
      " [ 0.          0.          0.94222224  0.05777778]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.24721603  0.75278395]\n",
      " [ 0.          0.          0.99333334  0.00666667]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.09111111  0.90888888]\n",
      " [ 0.          0.          0.61444443  0.38555557]\n",
      " [ 0.          0.36666667  0.63333333  0.        ]\n",
      " [ 0.01222222  0.10333333  0.88444442  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.64333332  0.35666665]\n",
      " [ 0.          0.05555556  0.94444442  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.01333333  0.05888889  0.92777777]\n",
      " [ 0.          0.07111111  0.55333334  0.37555555]\n",
      " [ 0.          0.00888889  0.98777777  0.00333333]\n",
      " [ 0.          0.          0.92444444  0.07555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.95999998  0.04      ]\n",
      " [ 0.          0.          0.93222225  0.06777778]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.06        0.10555556  0.83444446  0.        ]\n",
      " [ 0.          0.50777775  0.46888888  0.02333333]\n",
      " [ 0.          0.00333333  0.99666667  0.        ]\n",
      " [ 0.          0.00111111  0.99888891  0.        ]\n",
      " [ 0.          0.          0.99888891  0.00111111]\n",
      " [ 0.          0.14222223  0.85666668  0.00111111]\n",
      " [ 0.          0.02444444  0.27888888  0.69666666]\n",
      " [ 0.          0.          0.44333333  0.55666667]\n",
      " [ 0.          0.          0.16222222  0.83777779]\n",
      " [ 0.          0.          0.85888886  0.14111111]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.00888889  0.03555556  0.95555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.99888891  0.00111111]\n",
      " [ 0.          0.          0.40666667  0.5933333 ]\n",
      " [ 0.          0.          0.85111111  0.14888889]\n",
      " [ 0.          0.          0.8888889   0.11111111]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.76888889  0.23111111]\n",
      " [ 0.          0.          0.96777779  0.03222222]\n",
      " [ 0.          0.06555556  0.93444443  0.        ]\n",
      " [ 0.          0.          0.00666667  0.99333334]\n",
      " [ 0.          0.          0.90222222  0.09777778]\n",
      " [ 0.          0.          0.59111112  0.40888888]\n",
      " [ 0.          0.          0.92666668  0.07333333]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.41222224  0.51555556  0.07222223]\n",
      " [ 0.          0.          0.75777775  0.24222222]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.13666667  0.85777777  0.00555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.01666667  0.98333335  0.        ]\n",
      " [ 0.          0.          0.28999999  0.70999998]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.47        0.52999997]\n",
      " [ 0.          0.          0.20888889  0.79111111]\n",
      " [ 0.          0.          0.68777776  0.31222221]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.01444444  0.98555553]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.09222222  0.90777779]\n",
      " [ 0.          0.          0.83111113  0.16888888]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.47111112  0.52888888]\n",
      " [ 0.          0.          0.62111109  0.37888888]\n",
      " [ 0.05111111  0.27555555  0.67333335  0.        ]\n",
      " [ 0.          0.          0.93888891  0.06111111]\n",
      " [ 0.          0.          0.98888886  0.01111111]\n",
      " [ 0.          0.01333333  0.59222221  0.39444444]\n",
      " [ 0.          0.          0.89888889  0.10111111]\n",
      " [ 0.          0.          0.3611111   0.6388889 ]\n",
      " [ 0.          0.          0.93666667  0.06333333]\n",
      " [ 0.          0.          0.10444444  0.89555556]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.70666665  0.29333332]\n",
      " [ 0.          0.          0.06111111  0.93888891]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.39555556  0.60444444]\n",
      " [ 0.          0.04444445  0.95555556  0.        ]\n",
      " [ 0.02555555  0.04444445  0.93000001  0.        ]\n",
      " [ 0.          0.          0.95999998  0.04      ]\n",
      " [ 0.          0.00111111  0.94        0.05888889]\n",
      " [ 0.          0.00333333  0.99666667  0.        ]\n",
      " [ 0.          0.          0.06111111  0.93888891]\n",
      " [ 0.          0.45222223  0.54777777  0.        ]\n",
      " [ 0.03777778  0.10222222  0.86000001  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.20999999  0.79000002]\n",
      " [ 0.          0.          0.2588889   0.7411111 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x,y = accr_test_input_fn()\n",
    "st = tf.argmax(y,axis=1)\n",
    "curr = sess.run(x)\n",
    "out = sess.run(y)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook will be the auto-tuning routine that will find a single \n",
    "# dot window using the subimage classifier\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "# application logic will be added here\n",
    "def cnn_model_fn(features,labels,mode):\n",
    "    '''Model function for CNN'''\n",
    "    #input layer\n",
    "    input_layer = tf.cast(tf.reshape(features,[-1,30,30,1]),tf.float32)\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                            filters=32,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                            filters=64,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],strides=2)\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(inputs=pool2)\n",
    "    # dense output layer\n",
    "    out1 = tf.layers.dense(inputs=flat,units=1024,activation=tf.nn.relu)  \n",
    "    dropout1 = tf.layers.dropout(\n",
    "      inputs=out1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n",
    "    \n",
    "    out = tf.layers.dense(inputs=dropout1, units=4)\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # Calculate loss( for both TRAIN AND EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        loss = tf.losses.mean_squared_error(out,labels['prob'])\n",
    "\n",
    "    # Configure the training op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=1e-3,\n",
    "            optimizer=tf.train.AdamOptimizer)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions= {\n",
    "        \"prob\" : out,\n",
    "        \"states\" : tf.argmax(out,axis=1),\n",
    "    }\n",
    "    \n",
    "    # Returna  ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(mode=mode,predictions=predictions,loss=loss, train_op=train_op)\n",
    "dd_classifier = learn.Estimator(model_fn=cnn_model_fn,model_dir=\"/Users/ssk4/tensorflow_models/cnn_prob/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAETCAYAAAB5g3L4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b7bde10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing of the experimental data loading\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder_path = \"/Users/ssk4/Downloads/exp_data/\"\n",
    "files = glob.glob(data_folder_path + \"*.dat\")\n",
    "\n",
    "# Data format is V_LGD I_DC(nA) V_LGS I_AC(nA) t(sec)\n",
    "# The format of the loaded array is [num_points,5]\n",
    "dat = np.loadtxt(files[12])\n",
    "sub_size = 100\n",
    "grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "interpolated_data = scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest')\n",
    "\n",
    "xx, yy = np.meshgrid(grid_x,grid_y)\n",
    "plt.pcolor(xx,yy,interpolated_data)\n",
    "plt.xlabel(r'$V_{d1}$',fontsize=16)\n",
    "plt.ylabel(r'$V_{d2}$',fontsize=16)\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Current (arb. units)',fontsize=16)\n",
    "\n",
    "\n",
    "sub_size = 30\n",
    "grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "interpolated_data = np.abs(scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest'))\n",
    "interpolated_data = interpolated_data/np.max(interpolated_data)\n",
    "\n",
    "state = dd_classifier.predict(x=interpolated_data,as_iterable=False)\n",
    "plt.savefig(\"/Users/ssk4/Desktop/data1.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEBCAYAAABv4kJxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ca8a2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xbins = np.arange(4)\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "y = np.abs(state['prob'][0])\n",
    "y = y/np.sum(y)\n",
    "color = [\"g\" if x > 0.3 else \"r\" for x in y]\n",
    "\n",
    "ax.bar(xbins,y,color=color)\n",
    "ax.set_xticks(xbins)\n",
    "ax.set_ylabel(r'$\\alpha$ Probability',fontsize=16)\n",
    "ax.set_xticklabels(('ShortCircuit','QPC','SD','DD'),fontsize=16)\n",
    "plt.savefig(\"/Users/ssk4/Desktop/data1_bar.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2]\n",
      "1 [3]\n",
      "2 [3]\n",
      "3 [3]\n",
      "4 [3]\n",
      "5 [3]\n",
      "6 [3]\n",
      "7 [2]\n",
      "8 [2]\n",
      "9 [3]\n",
      "10 [3]\n",
      "11 [2]\n",
      "12 [3]\n",
      "13 [3]\n",
      "14 [3]\n",
      "15 [2]\n",
      "16 [3]\n",
      "17 [3]\n",
      "18 [3]\n",
      "19 [3]\n",
      "20 [3]\n",
      "21 [3]\n",
      "22 [3]\n",
      "23 [3]\n",
      "24 [3]\n",
      "25 [3]\n",
      "26 [3]\n",
      "27 [3]\n",
      "28 [3]\n",
      "29 [3]\n",
      "30 [3]\n",
      "31 [3]\n",
      "32 [3]\n",
      "33 [3]\n",
      "34 [3]\n",
      "35 [3]\n",
      "36 [2]\n",
      "37 [2]\n",
      "38 [2]\n",
      "39 [3]\n",
      "40 [3]\n",
      "41 [3]\n",
      "42 [3]\n",
      "43 [3]\n",
      "44 [3]\n",
      "45 [3]\n",
      "46 [2]\n",
      "47 [2]\n",
      "48 [2]\n",
      "49 [2]\n",
      "50 [3]\n",
      "51 [2]\n",
      "52 [3]\n",
      "53 [3]\n"
     ]
    }
   ],
   "source": [
    "# Testing of the experimental data loading\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "\n",
    "data_folder_path = \"/Users/ssk4/Downloads/exp_data/\"\n",
    "files = glob.glob(data_folder_path + \"*.dat\")\n",
    "\n",
    "# Data format is V_LGD I_DC(nA) V_LGS I_AC(nA) t(sec)\n",
    "# The format of the loaded array is [num_points,5]\n",
    "index = np.random.randint(len(files))\n",
    "\n",
    "for i in range(len(files)):\n",
    "\n",
    "    dat = np.loadtxt(files[i])\n",
    "\n",
    "    sub_size = 30\n",
    "    grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "    grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "    xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "    interpolated_data = scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest')\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib inline\n",
    "    #plt.pcolor(interpolated_data)\n",
    "\n",
    "    print(i,dd_classifier.predict(x=interpolated_data,as_iterable=False)['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x11e731c88>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6fce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat = np.loadtxt(files[53])\n",
    "\n",
    "sub_size = 30\n",
    "grid_x = np.linspace(np.min(dat[:,0]),np.max(dat[:,0]),sub_size)\n",
    "grid_y = np.linspace(np.min(dat[:,2]),np.max(dat[:,2]),sub_size)\n",
    "xx,yy = np.meshgrid(grid_x,grid_y)\n",
    "interpolated_data = scipy.interpolate.griddata((dat[:,0],dat[:,2]),dat[:,1],(xx, yy), method='nearest')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.pcolor(interpolated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0131284,  0.0130977,  0.0130165,  0.0130461,  0.0130579,\n",
       "         0.0130705,  0.0130673,  0.013014 ,  0.013067 ,  0.0130673,\n",
       "         0.0130742,  0.0130424,  0.0131063,  0.013041 ,  0.0130354,\n",
       "         0.0130415,  0.0130442,  0.0129011,  0.0129984,  0.0130359,\n",
       "         0.0130761,  0.0130623,  0.0130196,  0.0130593,  0.0130596,\n",
       "         0.0130447,  0.013058 ,  0.0130686,  0.0130626,  0.0130784],\n",
       "       [ 0.0130309,  0.0130342,  0.0130773,  0.013095 ,  0.0130655,\n",
       "         0.0130709,  0.0130834,  0.0130363,  0.0130766,  0.0130537,\n",
       "         0.012883 ,  0.0130093,  0.0130959,  0.0130351,  0.0130685,\n",
       "         0.0130667,  0.0130831,  0.0129268,  0.0130313,  0.0130226,\n",
       "         0.01302  ,  0.0130421,  0.0130307,  0.0130727,  0.0130674,\n",
       "         0.013017 ,  0.013053 ,  0.0130509,  0.0130441,  0.0130853],\n",
       "       [ 0.0130076,  0.0130482,  0.0130506,  0.0130161,  0.0130545,\n",
       "         0.0130134,  0.0130176,  0.0130303,  0.0130606,  0.0130398,\n",
       "         0.0130371,  0.0130427,  0.0130241,  0.0130324,  0.0130324,\n",
       "         0.0130261,  0.0129602,  0.0129288,  0.0129862,  0.0130262,\n",
       "         0.0130061,  0.0130101,  0.0130434,  0.0130382,  0.0130001,\n",
       "         0.0129817,  0.012998 ,  0.0130463,  0.0130678,  0.0130331],\n",
       "       [ 0.0129934,  0.013033 ,  0.0130116,  0.0130222,  0.0130083,\n",
       "         0.0130308,  0.013    ,  0.0130095,  0.0130049,  0.0129833,\n",
       "         0.012997 ,  0.0129974,  0.0129981,  0.0130548,  0.0130251,\n",
       "         0.0130171,  0.0129384,  0.0129166,  0.0130323,  0.013015 ,\n",
       "         0.0129939,  0.0129921,  0.0130225,  0.0130074,  0.0130633,\n",
       "         0.0129911,  0.0129835,  0.0130655,  0.0129995,  0.0129818],\n",
       "       [ 0.0129748,  0.0129416,  0.0129553,  0.0129737,  0.0129825,\n",
       "         0.0130094,  0.0129584,  0.0129495,  0.0129923,  0.0129904,\n",
       "         0.0129665,  0.0129505,  0.0129662,  0.0129914,  0.0129955,\n",
       "         0.0130118,  0.0128261,  0.0129328,  0.0129888,  0.0129225,\n",
       "         0.0129505,  0.012959 ,  0.0129742,  0.0129693,  0.0130199,\n",
       "         0.0129735,  0.0130046,  0.0129428,  0.0129986,  0.0129525],\n",
       "       [ 0.0129746,  0.0129743,  0.0129592,  0.012995 ,  0.0129782,\n",
       "         0.0129573,  0.0129753,  0.0129501,  0.0129708,  0.012984 ,\n",
       "         0.0129706,  0.0129467,  0.0129778,  0.0129432,  0.0129653,\n",
       "         0.012945 ,  0.0128224,  0.0129213,  0.012946 ,  0.012965 ,\n",
       "         0.0129304,  0.0129936,  0.0129768,  0.0129453,  0.0129717,\n",
       "         0.0129581,  0.0129242,  0.0129755,  0.0129609,  0.0129866],\n",
       "       [ 0.0129578,  0.012948 ,  0.0129179,  0.0128749,  0.0129351,\n",
       "         0.012935 ,  0.0129302,  0.0129051,  0.0128886,  0.012919 ,\n",
       "         0.012968 ,  0.0129925,  0.0129548,  0.0128705,  0.0128942,\n",
       "         0.0129102,  0.0127968,  0.0129172,  0.0129444,  0.0129032,\n",
       "         0.0129481,  0.0129385,  0.0129578,  0.0129613,  0.0129207,\n",
       "         0.0129127,  0.0128965,  0.012929 ,  0.0129213,  0.0129048],\n",
       "       [ 0.012873 ,  0.012869 ,  0.0128665,  0.0128723,  0.0128785,\n",
       "         0.0128748,  0.0128946,  0.0129008,  0.0128481,  0.0129099,\n",
       "         0.0128983,  0.0128809,  0.0128964,  0.0128874,  0.0129008,\n",
       "         0.0128325,  0.0127805,  0.0128962,  0.0129294,  0.0128569,\n",
       "         0.0129368,  0.012895 ,  0.0129053,  0.0129215,  0.0129003,\n",
       "         0.0128899,  0.0129159,  0.0128744,  0.0128794,  0.01287  ],\n",
       "       [ 0.0128763,  0.0129107,  0.0128922,  0.0128436,  0.0128826,\n",
       "         0.012865 ,  0.0128973,  0.0128964,  0.0128712,  0.0129035,\n",
       "         0.0128459,  0.0128768,  0.0128641,  0.0128703,  0.0129098,\n",
       "         0.0127678,  0.0128003,  0.0128784,  0.012824 ,  0.012861 ,\n",
       "         0.012845 ,  0.0128296,  0.0128422,  0.0128885,  0.0128645,\n",
       "         0.012848 ,  0.0128508,  0.0128341,  0.0128578,  0.0129282],\n",
       "       [ 0.0127861,  0.0128656,  0.0128201,  0.0128475,  0.0128328,\n",
       "         0.0128438,  0.0128728,  0.0128781,  0.0129004,  0.0128496,\n",
       "         0.0128932,  0.0128689,  0.0128535,  0.0128619,  0.012949 ,\n",
       "         0.0127467,  0.0128048,  0.0128431,  0.0128296,  0.0128422,\n",
       "         0.0128087,  0.012861 ,  0.0128229,  0.0128809,  0.0128761,\n",
       "         0.0129035,  0.0128798,  0.012853 ,  0.0128364,  0.0128212],\n",
       "       [ 0.0128094,  0.0128387,  0.0128222,  0.0128403,  0.0128598,\n",
       "         0.0128292,  0.0128313,  0.0128296,  0.0128334,  0.0128218,\n",
       "         0.0128164,  0.0128492,  0.0128185,  0.0129107,  0.0129379,\n",
       "         0.0126629,  0.0127871,  0.0128034,  0.0128278,  0.0128317,\n",
       "         0.0128162,  0.0128057,  0.0127978,  0.0127934,  0.0128721,\n",
       "         0.0128499,  0.0128489,  0.0128276,  0.0128192,  0.0128189],\n",
       "       [ 0.0127744,  0.0128512,  0.0127768,  0.0128556,  0.0128217,\n",
       "         0.0128101,  0.012824 ,  0.0127886,  0.0128386,  0.0127989,\n",
       "         0.012813 ,  0.0128512,  0.0128426,  0.0128844,  0.0129239,\n",
       "         0.0126378,  0.0127575,  0.0127717,  0.0128386,  0.012851 ,\n",
       "         0.0128161,  0.0128163,  0.0128231,  0.0128284,  0.0128428,\n",
       "         0.0128251,  0.0128054,  0.0128337,  0.0128012,  0.0128119],\n",
       "       [ 0.0127462,  0.0127675,  0.012792 ,  0.0127999,  0.0128061,\n",
       "         0.0128249,  0.012765 ,  0.0128007,  0.0127954,  0.0128128,\n",
       "         0.0128289,  0.0127589,  0.0127622,  0.0129028,  0.0128862,\n",
       "         0.0125541,  0.0127699,  0.012768 ,  0.0127733,  0.012807 ,\n",
       "         0.0127973,  0.0127806,  0.0127708,  0.0127798,  0.012778 ,\n",
       "         0.0128029,  0.0128138,  0.0128263,  0.0128451,  0.0128035],\n",
       "       [ 0.0127712,  0.0127884,  0.0127908,  0.0127682,  0.0127987,\n",
       "         0.012764 ,  0.0127693,  0.0127766,  0.0127717,  0.0127522,\n",
       "         0.0127621,  0.0128117,  0.0128251,  0.0130733,  0.0127873,\n",
       "         0.012526 ,  0.0127438,  0.0127705,  0.0128116,  0.0128179,\n",
       "         0.0127726,  0.0127956,  0.012808 ,  0.0127891,  0.0127647,\n",
       "         0.0127835,  0.0127701,  0.0127585,  0.0127714,  0.0127533],\n",
       "       [ 0.0127432,  0.0127565,  0.0127781,  0.0127623,  0.0127741,\n",
       "         0.0127834,  0.0127932,  0.0127795,  0.0127528,  0.0128059,\n",
       "         0.0128146,  0.0128601,  0.0129065,  0.0133745,  0.012508 ,\n",
       "         0.0123649,  0.0127365,  0.0127994,  0.0127699,  0.012756 ,\n",
       "         0.0127372,  0.0127462,  0.0127844,  0.012749 ,  0.0127309,\n",
       "         0.0127579,  0.0127843,  0.0127697,  0.0127771,  0.012756 ],\n",
       "       [ 0.0127394,  0.0127919,  0.0127431,  0.0128069,  0.0127863,\n",
       "         0.0127809,  0.0128148,  0.0127797,  0.012787 ,  0.0127495,\n",
       "         0.0128193,  0.0128414,  0.0130628,  0.0135273,  0.0118761,\n",
       "         0.0123234,  0.0127298,  0.0127628,  0.0127416,  0.0127279,\n",
       "         0.0127707,  0.0127226,  0.0127802,  0.0127726,  0.0127844,\n",
       "         0.0127818,  0.0127563,  0.0127452,  0.0127505,  0.0127619],\n",
       "       [ 0.0127692,  0.0127826,  0.0127462,  0.0127415,  0.0127315,\n",
       "         0.0127592,  0.0127389,  0.0127452,  0.012768 ,  0.0127559,\n",
       "         0.0127745,  0.0128891,  0.0130868,  0.0129237,  0.0114431,\n",
       "         0.0120937,  0.0127164,  0.0127692,  0.0127389,  0.0127364,\n",
       "         0.0127548,  0.0127466,  0.012732 ,  0.0127499,  0.0127454,\n",
       "         0.0127457,  0.0127524,  0.012757 ,  0.012771 ,  0.0127153],\n",
       "       [ 0.0127258,  0.0127267,  0.012749 ,  0.0127478,  0.0127028,\n",
       "         0.0127227,  0.0127192,  0.0127552,  0.0128814,  0.0127134,\n",
       "         0.0127764,  0.0128114,  0.0127221,  0.0111119,  0.0110629,\n",
       "         0.0122693,  0.0127021,  0.0127546,  0.0129188,  0.01273  ,\n",
       "         0.0127096,  0.0127367,  0.0127365,  0.0127308,  0.0127148,\n",
       "         0.0127415,  0.0126983,  0.0127504,  0.0127651,  0.0127604],\n",
       "       [ 0.0127   ,  0.0127279,  0.012747 ,  0.012723 ,  0.0127325,\n",
       "         0.0127176,  0.0127721,  0.012686 ,  0.0127527,  0.0127047,\n",
       "         0.0126373,  0.0125199,  0.011979 ,  0.0098454,  0.0109596,\n",
       "         0.0122546,  0.0126594,  0.0126761,  0.0127377,  0.0127039,\n",
       "         0.0127197,  0.0126944,  0.0127397,  0.0127154,  0.0127307,\n",
       "         0.0127556,  0.0126868,  0.0126914,  0.0127104,  0.0127107],\n",
       "       [ 0.0126757,  0.0126595,  0.0127273,  0.0127403,  0.0127364,\n",
       "         0.012731 ,  0.0127164,  0.0126458,  0.0126492,  0.0125614,\n",
       "         0.0124817,  0.0123596,  0.0120898,  0.0108086,  0.0112552,\n",
       "         0.0124907,  0.0126871,  0.0127122,  0.0126958,  0.0126909,\n",
       "         0.012679 ,  0.0127036,  0.012686 ,  0.0126925,  0.0127109,\n",
       "         0.0126715,  0.0126974,  0.0127313,  0.0126974,  0.0127452],\n",
       "       [ 0.0126883,  0.0126821,  0.0126818,  0.0126906,  0.0126855,\n",
       "         0.012664 ,  0.0125819,  0.0125603,  0.0125417,  0.0125733,\n",
       "         0.0125148,  0.0124795,  0.0123818,  0.0121939,  0.0122579,\n",
       "         0.0125497,  0.0126598,  0.0126595,  0.0127108,  0.012728 ,\n",
       "         0.0126737,  0.0127573,  0.0127397,  0.0126723,  0.012695 ,\n",
       "         0.0126948,  0.012728 ,  0.0127294,  0.0127315,  0.0127285],\n",
       "       [ 0.012672 ,  0.0127201,  0.0126486,  0.0126862,  0.0126467,\n",
       "         0.012617 ,  0.0126054,  0.0126428,  0.0125696,  0.0125952,\n",
       "         0.01262  ,  0.0125751,  0.0126193,  0.0125803,  0.0125319,\n",
       "         0.0126009,  0.0126755,  0.0127044,  0.0126718,  0.0127597,\n",
       "         0.0127854,  0.012795 ,  0.0127581,  0.0126967,  0.012729 ,\n",
       "         0.0127285,  0.0127344,  0.012738 ,  0.0126969,  0.0127006],\n",
       "       [ 0.0126774,  0.0126461,  0.0126698,  0.0126384,  0.0126549,\n",
       "         0.0126154,  0.0126665,  0.0126583,  0.0126656,  0.0126576,\n",
       "         0.0126544,  0.01263  ,  0.0126647,  0.0126335,  0.0126579,\n",
       "         0.0126356,  0.0126904,  0.0126505,  0.01269  ,  0.0128451,\n",
       "         0.0130313,  0.0130487,  0.0128942,  0.0127668,  0.0126916,\n",
       "         0.0126795,  0.0126395,  0.0126702,  0.0126863,  0.0126849],\n",
       "       [ 0.0126548,  0.0126481,  0.0126556,  0.0126123,  0.0126388,\n",
       "         0.0126363,  0.0126504,  0.0126023,  0.0126453,  0.0126189,\n",
       "         0.012692 ,  0.0126595,  0.0126454,  0.0126326,  0.0126505,\n",
       "         0.0126706,  0.0126512,  0.0126797,  0.012742 ,  0.0132744,\n",
       "         0.0138622,  0.0132521,  0.012746 ,  0.0126428,  0.0126707,\n",
       "         0.0126686,  0.0126402,  0.012616 ,  0.0126442,  0.0126326],\n",
       "       [ 0.0126089,  0.01259  ,  0.0126425,  0.0126643,  0.0126767,\n",
       "         0.0126341,  0.0125789,  0.0126208,  0.0126499,  0.0126395,\n",
       "         0.0126316,  0.0126121,  0.0126282,  0.0126447,  0.0126177,\n",
       "         0.0126481,  0.0126207,  0.0126477,  0.0128922,  0.0140598,\n",
       "         0.0141474,  0.0128116,  0.012608 ,  0.0125921,  0.0126271,\n",
       "         0.0125978,  0.0126125,  0.0126038,  0.0126141,  0.01259  ],\n",
       "       [ 0.0125901,  0.0126269,  0.0125943,  0.0125814,  0.0126225,\n",
       "         0.0125358,  0.0126492,  0.0126429,  0.0126452,  0.0126081,\n",
       "         0.0126306,  0.0126111,  0.0125888,  0.0126043,  0.0126268,\n",
       "         0.0126241,  0.0126176,  0.0126245,  0.0129793,  0.0142107,\n",
       "         0.0132262,  0.01227  ,  0.0123868,  0.0124762,  0.012533 ,\n",
       "         0.0125539,  0.0126125,  0.0125679,  0.012592 ,  0.0126236],\n",
       "       [ 0.0125978,  0.0126209,  0.0125658,  0.0125672,  0.0125746,\n",
       "         0.0126241,  0.0125751,  0.0125874,  0.0125597,  0.0126076,\n",
       "         0.0125964,  0.0125758,  0.012612 ,  0.0126099,  0.0125772,\n",
       "         0.0125964,  0.012638 ,  0.012639 ,  0.0127507,  0.0137987,\n",
       "         0.0099739,  0.0095994,  0.011493 ,  0.0121667,  0.0124389,\n",
       "         0.01249  ,  0.0125914,  0.0125806,  0.012599 ,  0.0126086],\n",
       "       [ 0.0125916,  0.0125744,  0.0125915,  0.0125925,  0.0125981,\n",
       "         0.0126034,  0.0126234,  0.0125686,  0.0125795,  0.0125592,\n",
       "         0.0126159,  0.0125751,  0.0125922,  0.0125618,  0.0125544,\n",
       "         0.0125711,  0.0126232,  0.0126301,  0.0128014,  0.0131585,\n",
       "         0.0094514,  0.0083262,  0.0113995,  0.0123537,  0.0124601,\n",
       "         0.0125323,  0.0125906,  0.0125779,  0.0126045,  0.012626 ],\n",
       "       [ 0.0125507,  0.0125516,  0.0125474,  0.0125235,  0.0125476,\n",
       "         0.0125704,  0.01254  ,  0.0125493,  0.0125579,  0.0126023,\n",
       "         0.0125451,  0.0125611,  0.0126204,  0.0126074,  0.012549 ,\n",
       "         0.0125672,  0.0125964,  0.0126261,  0.0127821,  0.0126227,\n",
       "         0.0103234,  0.0105495,  0.0121601,  0.0123994,  0.0125031,\n",
       "         0.0125295,  0.0125753,  0.0125718,  0.0126178,  0.0125499],\n",
       "       [ 0.012524 ,  0.0125383,  0.0125288,  0.0125514,  0.0125353,\n",
       "         0.0125161,  0.0125506,  0.0125455,  0.0125351,  0.012583 ,\n",
       "         0.0125181,  0.0125028,  0.0125233,  0.0125388,  0.0125277,\n",
       "         0.0125225,  0.0125453,  0.0125704,  0.0127861,  0.0122885,\n",
       "         0.011116 ,  0.0115803,  0.0123817,  0.0124644,  0.0125451,\n",
       "         0.0125153,  0.0125611,  0.0125393,  0.0125325,  0.0125569]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "export_savedmodel() missing 1 required positional argument: 'serving_input_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-675d41a2915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_savedmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/dd_classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: export_savedmodel() missing 1 required positional argument: 'serving_input_fn'"
     ]
    }
   ],
   "source": [
    "dd_classifier.export_savedmodel('/tmp/dd_classifier',serving_input_fn=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
